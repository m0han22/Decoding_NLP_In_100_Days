{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "690077f2-abb8-4454-bf48-72b9b96bcfbc",
   "metadata": {},
   "source": [
    "## NLP Basics\r\n",
    "\r\n",
    "### Concepts to Understand\r\n",
    "* Tokenization (Text Preprocessing)\r\n",
    "* Lemmatization (Text Preprocessing)\r\n",
    "* Stemming (Text Preprocessing)\r\n",
    "* Bag of Words (Text Preprocessing)\r\n",
    "* TFIDF (Term Frequency -Inverse Document Frequency) (Text Preprocessing)\r\n",
    "* Unigrams (Text Preprocessing) \r\n",
    "* Bigrams (Text Preprocessing) \r\n",
    "* Word2Vec (Text Preprocessing) \r\n",
    "* Average Word2Vec (Text Preprocessing) \r\n",
    "* RNN (Neural Networks) \r\n",
    "* LSTM RNN (Neural Networks) \r\n",
    "* GRU RNN (Neural Networks) \r\n",
    "* Word Embedding\r\n",
    "* Transformer\r\n",
    "* BERT\r\n",
    "ormer\r\n",
    "â€¢\tBERT\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bc0ef-a21a-44f1-a38f-ba4de041c5cd",
   "metadata": {},
   "source": [
    "# Terminology\n",
    "1.\tCorpus: Paragraphs\r\n",
    "2.\tDocuments: Sentences\r\n",
    "3.\tVocabulary: Unique Words\r\n",
    "4.\tWords: All Words in a Corpus\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75a198c-9b31-404f-819f-c54d6ac5c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '''Hi My name is Sai Mohan, Working at Spire Solutions.\n",
    "I am currently working on NLP, from Krish Naik Show on Youtube.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97f364f6-ce54-491b-96a4-e160242c43c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi My name is Sai Mohan, Working at Spire Solutions.\n",
      "I am currently working on NLP, from Krish Naik Show on Youtube.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d0ea887-5489-4e66-9053-0f9fea499a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# Convert Sentence into Paragrphs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# sent_tokenize converts paragraph into sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f428afe-e734-4a0c-a997-6f7518320669",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5f28df3-b5a0-404b-8f08-529d46fdd68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi My name is Sai Mohan, Working at Spire Solutions.',\n",
       " 'I am currently working on NLP, from Krish Naik Show on Youtube.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02103f2-0979-492d-80a3-a47f21282010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization \n",
    "# Paragraph into words\n",
    "# sentence into words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "886185a4-5cec-4131-bdce-e463600e6d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Sai',\n",
       " 'Mohan',\n",
       " ',',\n",
       " 'Working',\n",
       " 'at',\n",
       " 'Spire',\n",
       " 'Solutions',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'working',\n",
       " 'on',\n",
       " 'NLP',\n",
       " ',',\n",
       " 'from',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " 'Show',\n",
       " 'on',\n",
       " 'Youtube',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f6ab171-5a8e-4da3-88f2-2c52fac64d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "593ef1a0-20b7-4fbe-9edf-b262be29382f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Sai',\n",
       " 'Mohan',\n",
       " ',',\n",
       " 'Working',\n",
       " 'at',\n",
       " 'Spire',\n",
       " 'Solutions',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'working',\n",
       " 'on',\n",
       " 'NLP',\n",
       " ',',\n",
       " 'from',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " 'Show',\n",
       " 'on',\n",
       " 'Youtube',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b725bb-a20a-4924-b9a3-b7075dfb784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa97a61a-0078-40c7-834b-5e186ae40c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Sai',\n",
       " 'Mohan',\n",
       " ',',\n",
       " 'Working',\n",
       " 'at',\n",
       " 'Spire',\n",
       " 'Solutions.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'working',\n",
       " 'on',\n",
       " 'NLP',\n",
       " ',',\n",
       " 'from',\n",
       " 'Krish',\n",
       " 'Naik',\n",
       " 'Show',\n",
       " 'on',\n",
       " 'Youtube',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TreebankWordTokenizer().tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ca193-5b54-40b5-90d4-5f61172849bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
